{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sgmcart3\\AppData\\Roaming\\Python\\Python38\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import gensim\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from bert_serving.client import BertClient\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.getcwd().split(\"src\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# papers, ref_submissions, industrial_strategy\n",
    "document_type = \"papers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_file(data, file):\n",
    "    '''\n",
    "    Write data to JSON file\n",
    "    '''\n",
    "\n",
    "    with open(f\"{file}\", \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file):\n",
    "    '''\n",
    "    Read data from JSON file\n",
    "    '''\n",
    "\n",
    "    with open(f\"{file}\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Preprocessed JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1641 documents\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "json_documents = glob.glob(f\"{ROOT}/data/{document_type}/json/preprocessed/**/*.json\",\n",
    "                          recursive=True)\n",
    "for json_document in json_documents:\n",
    "    data = read_json_file(f\"{json_document}\")\n",
    "    if \"body\" in data.keys():\n",
    "        document_string = \" \".join(data[\"body\"])\n",
    "    elif \"chapters\" in data.keys():\n",
    "        document_string = \"\"\n",
    "        for chapter in data[\"chapters\"].keys():\n",
    "            document_string += f\" {data['chapters'][chapter]}\"\n",
    "    corpus.append(document_string)\n",
    "print(f\"Loaded {len(corpus)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_X = count_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "count_vectorised = {}\n",
    "for json_document in json_documents:\n",
    "    file_name = json_document.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "    data = read_json_file(f\"{json_document}\")\n",
    "    if \"body\" in data.keys():\n",
    "        document_string = \" \".join(data[\"body\"])\n",
    "    elif \"chapters\" in data.keys():\n",
    "        document_string = \"\"\n",
    "        for chapter in data[\"chapters\"].keys():\n",
    "            document_string += f\" {data['chapters'][chapter]}\"\n",
    "    tokens = count_vectorizer.transform([document_string]).toarray().tolist()\n",
    "    count_vectorised[file_name] = {}\n",
    "    if document_type == \"papers\":\n",
    "        count_vectorised[file_name][\"tokens\"] = tokens\n",
    "        if \"arxiv\" in json_document:\n",
    "            count_vectorised[file_name][\"database\"] = \"arxiv\"\n",
    "        elif \"scopus\" in json_document:\n",
    "            count_vectorised[file_name][\"database\"] = \"scopus\"\n",
    "        else:\n",
    "            count_vectorised[file_name][\"database\"] = \"unknown\"\n",
    "    else:\n",
    "        count_vectorised[file_name] = tokens\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_file(count_vectorised, f\"{ROOT}/data/{document_type}/json/vectorised/count_vectors.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "tfidf_X = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorised = {}\n",
    "for json_document in json_documents:\n",
    "    file_name = json_document.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "    data = read_json_file(f\"{json_document}\")\n",
    "    if \"body\" in data.keys():\n",
    "        document_string = \" \".join(data[\"body\"])\n",
    "    elif \"chapters\" in data.keys():\n",
    "        document_string = \"\"\n",
    "        for chapter in data[\"chapters\"].keys():\n",
    "            document_string += f\" {data['chapters'][chapter]}\"\n",
    "    tokens = tfidf_vectorizer.transform([document_string]).toarray().tolist()\n",
    "    tfidf_vectorised[file_name] = {}\n",
    "    if document_type == \"papers\":\n",
    "        tfidf_vectorised[file_name][\"tokens\"] = tokens\n",
    "        if \"arxiv\" in json_document:\n",
    "            tfidf_vectorised[file_name][\"database\"] = \"arxiv\"\n",
    "        elif \"scopus\" in json_document:\n",
    "            tfidf_vectorised[file_name][\"database\"] = \"scopus\"\n",
    "        else:\n",
    "            tfidf_vectorised[file_name][\"database\"] = \"unknown\"\n",
    "    else:\n",
    "        tfidf_vectorised[file_name] = tokens\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_file(tfidf_vectorised, f\"{ROOT}/data/{document_type}/json/vectorised/tfidf_vectors.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "corpus = api.load('text8')\n",
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise_sentence(model, sentence):\n",
    "    sentence_vectors = []\n",
    "    tokens = sentence.split(\" \")\n",
    "    num_tokens = 0\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            if num_tokens == 0:\n",
    "                sentence_vectors = model[token]\n",
    "            else:\n",
    "                sentence_vectors = np.add(sentence_vectors,\n",
    "                                          model[token])\n",
    "            num_tokens += 1\n",
    "        except:\n",
    "            pass\n",
    "    if num_tokens == 0:\n",
    "        return np.array([])\n",
    "    return sentence_vectors / num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 complete\n",
      "6.0938452163315056 complete\n",
      "12.187690432663011 complete\n",
      "18.281535648994517 complete\n",
      "24.375380865326022 complete\n",
      "30.469226081657524 complete\n",
      "36.56307129798903 complete\n",
      "42.656916514320535 complete\n",
      "48.750761730652044 complete\n",
      "54.844606946983546 complete\n",
      "60.93845216331505 complete\n",
      "67.03229737964655 complete\n",
      "73.12614259597807 complete\n",
      "79.21998781230957 complete\n",
      "85.31383302864107 complete\n",
      "91.40767824497257 complete\n",
      "97.50152346130409 complete\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "word2vec_vectorised = {}\n",
    "for i, json_document in enumerate(json_documents):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i / len(json_documents) * 100} complete\")\n",
    "    file_name = json_document.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "    data = read_json_file(f\"{json_document}\")\n",
    "    tokenized_sentences = []\n",
    "    num_sentences = 0\n",
    "    if \"body\" in data.keys():\n",
    "        for sentence in data[\"body\"]:\n",
    "            try:\n",
    "                if num_sentences == 0:\n",
    "                    tokenized_sentences = vectorise_sentence(model, sentence)\n",
    "                else:\n",
    "                    tokenized_sentences = np.add(tokenized_sentences,\n",
    "                                                 vectorise_sentence(model, sentence))\n",
    "                num_sentences += 1\n",
    "            except:\n",
    "                pass\n",
    "    elif \"chapters\" in data.keys():\n",
    "        for chapter in data[\"chapters\"]:\n",
    "            for sentence in data[\"chapters\"][chapter]:\n",
    "                try:\n",
    "                    if num_sentences == 0:\n",
    "                        tokenized_sentences = vectorise_sentence(model, sentence)\n",
    "                    else:\n",
    "                        tokenized_sentences = np.add(tokenized_sentences,\n",
    "                                                     vectorise_sentence(model, sentence))\n",
    "                    num_sentences += 1\n",
    "                except:\n",
    "                    pass\n",
    "    if num_sentences == 0:\n",
    "        continue\n",
    "    tokenized_sentences = tokenized_sentences / num_sentences\n",
    "    word2vec_vectorised[file_name] = {}\n",
    "    if document_type == \"papers\":\n",
    "        word2vec_vectorised[file_name][\"tokens\"] = tokenized_sentences.tolist()\n",
    "        if \"arxiv\" in json_document:\n",
    "            word2vec_vectorised[file_name][\"database\"] = \"arxiv\"\n",
    "        elif \"scopus\" in json_document:\n",
    "            word2vec_vectorised[file_name][\"database\"] = \"scopus\"\n",
    "        else:\n",
    "            word2vec_vectorised[file_name][\"database\"] = \"unknown\"\n",
    "    else:\n",
    "        word2vec_vectorised[file_name] = tokenized_sentences.tolist()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_file(word2vec_vectorised, f\"{ROOT}/data/{document_type}/json/vectorised/word2vec_vectors.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4d68dd581071>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnum_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"body\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mtokenized_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"body\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mnum_sentences\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;34m\"chapters\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\bert_serving\\client\\__init__.py\u001b[0m in \u001b[0;36marg_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreceiver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzmq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRCVTIMEO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAgain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_e\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m                 t_e = TimeoutError(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\bert_serving\\client\\__init__.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, texts, blocking, is_tokenized, show_tokens)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recv_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_info_available\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mshow_tokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\bert_serving\\client\\__init__.py\u001b[0m in \u001b[0;36m_recv_ndarray\u001b[1;34m(self, wait_for_req_id)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_recv_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwait_for_req_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mrequest_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait_for_req_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[0marr_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjsonapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dtype'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\bert_serving\\client\\__init__.py\u001b[0m in \u001b[0;36m_recv\u001b[1;34m(self, wait_for_req_id)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                 \u001b[1;31m# receive a response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                 \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreceiver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m                 \u001b[0mrequest_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[1;34m(self, flags, copy, track)\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0many\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmight\u001b[0m \u001b[0mfail\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \"\"\"\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m         \u001b[1;31m# have first part already, only loop while more to receive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsockopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzmq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRCVMORE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_vectorised = {}\n",
    "for i, json_document in enumerate(json_documents):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i / len(json_documents) * 100} complete\")\n",
    "    file_name = json_document.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "    data = read_json_file(f\"{json_document}\")\n",
    "    tokenized_sentences = []\n",
    "    num_sentences = 0\n",
    "    if \"body\" in data.keys():\n",
    "        tokenized_sentences = bc.encode(data[\"body\"])\n",
    "        num_sentences += 1\n",
    "    elif \"chapters\" in data.keys():\n",
    "        for chapter in data[\"chapters\"]:\n",
    "            if num_sentences == 0:\n",
    "                tokenized_sentences = bc.encode(data[\"body\"])\n",
    "            else:\n",
    "                tokenized_sentences = np.add(tokenized_sentences,\n",
    "                                             bc.encode(data[\"body\"]))\n",
    "                \n",
    "            num_sentences += 1\n",
    "    if num_sentences == 0:\n",
    "        continue\n",
    "    tokenized_sentences = tokenized_sentences / num_sentences\n",
    "    bert_vectorised[file_name] = {}\n",
    "    if document_type == \"papers\":\n",
    "        bert_vectorised[file_name][\"tokens\"] = tokenized_sentences.tolist()\n",
    "        if \"arxiv\" in json_document:\n",
    "            bert_vectorised[file_name][\"database\"] = \"arxiv\"\n",
    "        elif \"scopus\" in json_document:\n",
    "            bert_vectorised[file_name][\"database\"] = \"scopus\"\n",
    "        else:\n",
    "            bert_vectorised[file_name][\"database\"] = \"unknown\"\n",
    "    else:\n",
    "        bert_vectorised[file_name] = tokenized_sentences.tolist()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\sgmcart3\\\\Documents\\\\Projects\\\\target_app\\\\/data/papers/json/vectorised/bert_vectors.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-80738b8bd03a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwrite_json_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_vectorised\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{ROOT}/data/{document_type}/json/vectorised/bert_vectors.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-6a3b667d63e5>\u001b[0m in \u001b[0;36mwrite_json_file\u001b[1;34m(data, file)\u001b[0m\n\u001b[0;32m      4\u001b[0m     '''\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{file}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\sgmcart3\\\\Documents\\\\Projects\\\\target_app\\\\/data/papers/json/vectorised/bert_vectors.json'"
     ]
    }
   ],
   "source": [
    "write_json_file(bert_vectorised, f\"{ROOT}/data/{document_type}/json/vectorised/bert_vectors.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
